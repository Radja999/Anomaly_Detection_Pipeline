{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**1 – Imports and Paths**"
      ],
      "metadata": {
        "id": "mZUqBPHbVV8v"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7RSh9pe9VQXo"
      },
      "outputs": [],
      "source": [
        "# ===============================\n",
        "# STEP 1 – Imports and Directory Setup\n",
        "# ===============================\n",
        "import os\n",
        "import gc\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Project paths\n",
        "BASE_DIR = \"Anomaly_Detection_Pipeline\"\n",
        "DATA_DIR = os.path.join(BASE_DIR, \"data\", \"processed\")\n",
        "\n",
        "PUBLIC_BENIGN_FILE = os.path.join(DATA_DIR, \"public_benign_set.csv\")\n",
        "PRIVATE_BENIGN_FILE = os.path.join(DATA_DIR, \"private_benign_set.csv\")\n",
        "ATTACK_FILE = os.path.join(DATA_DIR, \"attack_set.csv\")\n",
        "\n",
        "MODEL_DIR = \"Anomaly_Detection_Pipeline/models\"\n",
        "os.makedirs(MODEL_DIR, exist_ok=True)\n",
        "\n",
        "# -------------------------------\n",
        "# Load preprocessed data\n",
        "# -------------------------------\n",
        "df = pd.read_csv(PUBLIC_BENIGN_FILE)\n",
        "attack_df = pd.read_csv(ATTACK_FILE)\n",
        "\n",
        "# Reproducibility\n",
        "SEED = 42\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2 – Feature Groups**"
      ],
      "metadata": {
        "id": "QYpAr9iMVhn9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================\n",
        "# STEP 2 – Feature Groups (auto-filtered, constant columns removed)\n",
        "# ===============================\n",
        "cat_feature = \"Destination Port\"\n",
        "\n",
        "# Original no-scale candidates — now excluding the 8 constant columns\n",
        "candidate_no_scale = [\n",
        "    \"Fwd PSH Flags\", \"URG Flag Count\",\n",
        "    \"ECE Flag Count\", \"Active Max\", \"Active Min\",\n",
        "    \"Active Mean\", \"Active Std\", \"Idle Max\",\n",
        "    \"Idle Min\", \"Idle Mean\", \"Idle Std\", \"CWE Flag Count\"\n",
        "]\n",
        "\n",
        "# Keep only those that exist in the dataset\n",
        "no_scale_features = [f for f in candidate_no_scale if f in df.columns]\n",
        "\n",
        "missing = [f for f in [cat_feature] if f not in df.columns]\n",
        "if missing:\n",
        "    raise ValueError(f\"Missing required categorical feature(s): {missing}\")\n",
        "\n",
        "# Continuous features for scaling\n",
        "minmax_features = [\n",
        "    c for c in df.columns\n",
        "    if c not in no_scale_features + [cat_feature, \"Label\"]\n",
        "]\n",
        "\n",
        "print(f\" Dataset shape: {df.shape}\")\n",
        "print(f\" Robust MinMax features: {len(minmax_features)}\")\n",
        "print(f\" No-scaling features: {len(no_scale_features)}\")\n",
        "print(f\" Categorical feature: {cat_feature}\\n\")\n"
      ],
      "metadata": {
        "id": "zgfHtGCbVhx9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***3 – Port Encoding***"
      ],
      "metadata": {
        "id": "5Wq2ag5GVqIQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================\n",
        "# STEP 3 – Universal Port Encoding\n",
        "# ===============================\n",
        "cat_feature = \"Destination Port\"\n",
        "df[cat_feature] = df[cat_feature].astype(int)\n",
        "\n",
        "n_ports = 65536  # 0–65535\n",
        "print(f\"Using universal port embedding space of size {n_ports}.\")\n",
        "print(f\"Observed unique ports: {df[cat_feature].nunique()}\")\n",
        "print(f\"Port range: [{df[cat_feature].min()}, {df[cat_feature].max()}]\")\n"
      ],
      "metadata": {
        "id": "1FpuqJ9eVqct"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4 – Data Splitting**"
      ],
      "metadata": {
        "id": "bVccg5YDV18I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================\n",
        "# STEP 4 – Train / Validation / Test Split\n",
        "# ===============================\n",
        "train_df, test_df = train_test_split(df, test_size=0.2, random_state=SEED, shuffle=True)\n",
        "train_df, val_df = train_test_split(train_df, test_size=0.2, random_state=SEED, shuffle=True)\n",
        "\n",
        "print(f\"Train: {len(train_df)} | Val: {len(val_df)} | Test: {len(test_df)}\")\n"
      ],
      "metadata": {
        "id": "PJ6NeS4PV2PD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5 – Log Scaling**"
      ],
      "metadata": {
        "id": "VKSnAYB6V8Gt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================\n",
        "# STEP 5 – Log Scaling for Skewed Features\n",
        "# ===============================\n",
        "def log_safe(x):\n",
        "    return np.log1p(np.clip(x, a_min=0, a_max=None))\n",
        "\n",
        "def apply_log_scaling(df, features):\n",
        "    df_scaled = df.copy()\n",
        "    for col in features:\n",
        "        df_scaled[col] = log_safe(df[col].values)\n",
        "    return df_scaled\n",
        "\n",
        "train_scaled = apply_log_scaling(train_df, minmax_features)\n",
        "val_scaled   = apply_log_scaling(val_df, minmax_features)\n",
        "test_scaled  = apply_log_scaling(test_df, minmax_features)\n",
        "\n",
        "for col in no_scale_features:\n",
        "    if col in df.columns:\n",
        "        train_scaled[col] = train_df[col].astype(np.float32)\n",
        "        val_scaled[col]   = val_df[col].astype(np.float32)\n",
        "        test_scaled[col]  = test_df[col].astype(np.float32)\n",
        "\n",
        "train_scaled[cat_feature] = train_df[cat_feature].astype(np.int64)\n",
        "val_scaled[cat_feature]   = val_df[cat_feature].astype(np.int64)\n",
        "test_scaled[cat_feature]  = test_df[cat_feature].astype(np.int64)\n",
        "\n",
        "print(\"Log-scaling applied to continuous features.\")\n"
      ],
      "metadata": {
        "id": "RC95T0H2V8ug"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**6 – Save Schema and Bounds**"
      ],
      "metadata": {
        "id": "3EBwK7J1WGAR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================\n",
        "# STEP 6 – Save Schema and Bounds\n",
        "# ===============================\n",
        "cont_cols = [c for c in train_scaled.columns if c != cat_feature]\n",
        "\n",
        "bounds = {}\n",
        "for col in minmax_features:\n",
        "    if col in train_scaled.columns:\n",
        "        lower = 0.0\n",
        "        upper = train_scaled[col].quantile(0.999)\n",
        "        bounds[col] = (lower, upper)\n",
        "\n",
        "with open(os.path.join(MODEL_DIR, \"schema.pkl\"), \"wb\") as f:\n",
        "    pickle.dump({\n",
        "        \"cat_feature\": cat_feature,\n",
        "        \"cont_cols\": cont_cols,\n",
        "        \"minmax_features\": minmax_features,\n",
        "        \"no_scale_features\": no_scale_features\n",
        "    }, f)\n",
        "\n",
        "with open(os.path.join(MODEL_DIR, \"bounds.pkl\"), \"wb\") as f:\n",
        "    pickle.dump(bounds, f)\n",
        "\n",
        "print(f\"Saved schema ({len(cont_cols)} continuous cols) and bounds ({len(bounds)}).\")\n"
      ],
      "metadata": {
        "id": "MN9gjwy1WGNG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**7 – Convert to Torch Tensors**"
      ],
      "metadata": {
        "id": "d2omiySOWOSz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================\n",
        "# STEP 7 – Convert to Torch Tensors\n",
        "# ===============================\n",
        "def df_to_tensor(df_in):\n",
        "    cat = torch.as_tensor(df_in[cat_feature].values, dtype=torch.long)\n",
        "    cont = torch.as_tensor(df_in.drop(columns=[cat_feature]).values, dtype=torch.float32)\n",
        "    return cat, cont\n",
        "\n",
        "cat_train, cont_train = df_to_tensor(train_scaled)\n",
        "cat_val, cont_val = df_to_tensor(val_scaled)\n",
        "cat_test, cont_test = df_to_tensor(test_scaled)\n",
        "\n",
        "train_ds = TensorDataset(cat_train, cont_train)\n",
        "val_ds = TensorDataset(cat_val, cont_val)\n",
        "test_ds = TensorDataset(cat_test, cont_test)\n",
        "\n",
        "BATCH_SIZE = 4096 if device.type == \"cuda\" else 1024\n",
        "train_loader = DataLoader(train_ds, shuffle=True, batch_size=BATCH_SIZE, num_workers=2)\n",
        "val_loader = DataLoader(val_ds, shuffle=False, batch_size=BATCH_SIZE, num_workers=2)\n",
        "test_loader = DataLoader(test_ds, shuffle=False, batch_size=BATCH_SIZE, num_workers=2)\n",
        "\n",
        "print(f\"Data prepared for training. Batch size: {BATCH_SIZE}\")\n"
      ],
      "metadata": {
        "id": "gxarZNCwWOky"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**8 – Model Definition**"
      ],
      "metadata": {
        "id": "BamhI8pdWVAC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================\n",
        "# STEP 8 – Deep Autoencoder Definition\n",
        "# ===============================\n",
        "class DeepAutoencoder(nn.Module):\n",
        "    def __init__(self, n_cont, max_ports=65535, emb_dim=128, latent_dim=128):\n",
        "        super().__init__()\n",
        "        self.emb = nn.Embedding(num_embeddings=max_ports + 1, embedding_dim=emb_dim)\n",
        "        in_dim = n_cont + emb_dim\n",
        "\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(in_dim, 1024),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(1024, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, latent_dim)\n",
        "        )\n",
        "\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(latent_dim, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 1024),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(1024, n_cont)\n",
        "        )\n",
        "\n",
        "    def forward(self, cat, cont):\n",
        "        emb = self.emb(cat)\n",
        "        x = torch.cat([emb, cont], dim=1)\n",
        "        z = self.encoder(x)\n",
        "        recon = self.decoder(z)\n",
        "        return recon\n",
        "\n",
        "n_cont = len(df.columns) - 1\n",
        "model = DeepAutoencoder(n_cont, max_ports=65535, emb_dim=128, latent_dim=128).to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=5e-4, weight_decay=1e-5)\n",
        "criterion = nn.MSELoss()\n",
        "print(\"Model initialized successfully.\")\n"
      ],
      "metadata": {
        "id": "yK_WyElaWVQA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**9 – Training and Visualization**"
      ],
      "metadata": {
        "id": "RQViIQc8Wbwh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================\n",
        "# STEP 9 – Training Loop with Early Stopping\n",
        "# ===============================\n",
        "EPOCHS = 255\n",
        "PATIENCE = 20\n",
        "best_val_loss = float(\"inf\")\n",
        "epochs_no_improve = 0\n",
        "train_losses, val_losses, grad_norms = [], [], []\n",
        "\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    model.train()\n",
        "    total_train_loss, total_grad_norm = 0, 0\n",
        "\n",
        "    for cat, cont in train_loader:\n",
        "        cat, cont = cat.to(device), cont.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(cat, cont)\n",
        "        loss = criterion(output, cont)\n",
        "        loss.backward()\n",
        "        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n",
        "        total_grad_norm += grad_norm.item()\n",
        "        optimizer.step()\n",
        "        total_train_loss += loss.item() * cont.size(0)\n",
        "\n",
        "    total_train_loss /= len(train_loader.dataset)\n",
        "    avg_grad_norm = total_grad_norm / len(train_loader)\n",
        "    train_losses.append(total_train_loss)\n",
        "    grad_norms.append(avg_grad_norm)\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    total_val_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for cat, cont in val_loader:\n",
        "            cat, cont = cat.to(device), cont.to(device)\n",
        "            output = model(cat, cont)\n",
        "            total_val_loss += criterion(output, cont).item() * cont.size(0)\n",
        "    total_val_loss /= len(val_loader.dataset)\n",
        "    val_losses.append(total_val_loss)\n",
        "\n",
        "    print(f\"Epoch {epoch:03d}/{EPOCHS} | Train MSE: {total_train_loss:.6f} | Val MSE: {total_val_loss:.6f}\")\n",
        "\n",
        "    if total_val_loss < best_val_loss:\n",
        "        best_val_loss = total_val_loss\n",
        "        torch.save(model.state_dict(), os.path.join(MODEL_DIR, \"best_autoencoder.pt\"))\n",
        "        epochs_no_improve = 0\n",
        "    else:\n",
        "        epochs_no_improve += 1\n",
        "        if epochs_no_improve >= PATIENCE:\n",
        "            print(\"Early stopping triggered.\")\n",
        "            break\n",
        "\n",
        "print(f\"Training complete. Best Val MSE: {best_val_loss:.6f}\")\n",
        "\n",
        "# --- Plot loss curves ---\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.plot(train_losses, label='Train Loss')\n",
        "plt.plot(val_losses, label='Validation Loss')\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"MSE Loss\")\n",
        "plt.title(\"Training and Validation Loss over Epochs\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "N1d8FygNWcF4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}